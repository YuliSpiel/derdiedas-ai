{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc2d9f3",
   "metadata": {},
   "source": [
    "- CEFR(ìœ ëŸ½ ì–¸ì–´ ê³µí†µ ê¸°ì¤€, Common European Framework of Reference for Languages) ê¸°ë°˜ \n",
    "- ì‚¬ìš©ì ì–´í•™ìˆ˜ì¤€ í‰ê°€ ê¸°ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f4d03",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "- ì‹¤ì œ í•´ë‹¹ ë ˆë²¨ì˜ í•™ìŠµìê°€ ì–´ë–¤ ë¬¸ì¥ì„ êµ¬ì‚¬í•˜ëŠ”ì§€ ì •ë¦¬ëœ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03483f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í”„ë¡œì íŠ¸ ìºì‹œ í™œì„±í™”: /Users/yuli/Documents/AI_Workspace/derdiedas-ai.ai/notebooks/models_cache\n",
      "ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: ./models_cache\n",
      "í™˜ê²½ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ê²½ê³  ë¹„í™œì„±í™”\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ìºì‹œ ì„¤ì •\n",
    "project_cache = \"./models_cache\"\n",
    "os.environ[\"HF_HOME\"] = project_cache\n",
    "print(f\"í”„ë¡œì íŠ¸ ìºì‹œ í™œì„±í™”: {os.path.abspath(project_cache)}\")\n",
    "\n",
    "# Tokenizers ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë¹„í™œì„±í™”\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "cache_dir = os.environ.get(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {cache_dir}\")\n",
    "\n",
    "print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4588c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'lang', 'source_name', 'format', 'category', 'cefr_level', 'license', 'text'],\n",
      "    num_rows: 1033\n",
      "})\n",
      "{'title': '1023_0001416.txt', 'lang': 'de', 'source_name': 'merlin-de', 'format': 'paragraph-level', 'category': 'learner', 'cefr_level': 'B2', 'license': 'CC BY-SA 4.0', 'text': 'M. Meier\\nMÃ¼llergasse 1\\n12345 Stadt X\\nInternationale Au-pair Vermittlung\\nBahnhofstr. 101\\n65185 Wiesbaden\\n                                   Stadt X, den 14.05.11.\\nSehr geehrte Damen und Herren,\\nihre Anzeige in der Zeitung hat mich sehr erfreut, so was habe ich schon lange gesucht. \\nDa mir die Arbeit mit anderen Menschen sehr groÃŸen Spas bereitet, habe ich mich entschlossen, mich zu bewerben.\\nIch bin flexibel und fÃ¼r neuen Aufgaben ofen.\\nTakt, Geduld, warmherzigkeit und ZuverlÃ¤ssigkeit bringe ich mit, sowie Erfahrung im Umgang mit Kindern und der HaushaltsfÃ¼hrung.\\nIch spreche Russisch, Englisch, Kasachisch, habe Grundkenntnisse in Deutsch. ZusÃ¤tzlich habe ich Computerkenntnisse.\\nNaturlich wÃ¼rde ich gern Deutsche Sprache und das Leben in Deutschland kennenlernen. Ich wÃ¤re Ihnen sehr dankbar, wenn Sie mir weitere Informationen Ã¼ber Reche, Pflichen und AufenthaltsformalitÃ¤ten zuschicken kÃ¶nnten.\\nWerde ich versichert? Welche FreizeitaktivitÃ¤ten finden statt? Wiviel Stunden in der Woche mÃ¼ssen die Kinder betreut werden? Ob ich was verdiene?\\nÃœber eine baldige Antwort wÃ¼rde ich mich sehr freuen\\nMit freundlichen GrÃ¼ssen\\nMaria Meier'}\n",
      "\n",
      "=== UniversalCEFR ë°ì´í„°ì…‹ ì •ë³´ ===\n",
      "ì´ ìƒ˜í”Œ ìˆ˜: 1,033ê°œ\n",
      "ì»¬ëŸ¼: ['title', 'lang', 'source_name', 'format', 'category', 'cefr_level', 'license', 'text']\n",
      "\n",
      "ì²« ë²ˆì§¸ ìƒ˜í”Œ:\n",
      "  ì œëª©: 1023_0001416.txt\n",
      "  ì–¸ì–´: de\n",
      "  ì¹´í…Œê³ ë¦¬: learner\n",
      "  ë ˆë²¨: B2\n",
      "  í…ìŠ¤íŠ¸: M. Meier\n",
      "MÃ¼llergasse 1\n",
      "12345 Stadt X\n",
      "Internationale Au-pair Vermittlung\n",
      "Bahnhofstr. 101\n",
      "65185 Wiesbaden\n",
      "                                   Stadt X, den 14.05.11.\n",
      "Sehr geehrte Damen und Herren,\n",
      "ihre Anzeige in der Zeitung hat mich sehr erfreut, so was habe ich schon lange gesucht. \n",
      "Da mir die Arbeit mit anderen Menschen sehr groÃŸen Spas bereitet, habe ich mich entschlossen, mich zu bewerben.\n",
      "Ich bin flexibel und fÃ¼r neuen Aufgaben ofen.\n",
      "Takt, Geduld, warmherzigkeit und ZuverlÃ¤ssigkeit bringe ich mit, sowie Erfahrung im Umgang mit Kindern und der HaushaltsfÃ¼hrung.\n",
      "Ich spreche Russisch, Englisch, Kasachisch, habe Grundkenntnisse in Deutsch. ZusÃ¤tzlich habe ich Computerkenntnisse.\n",
      "Naturlich wÃ¼rde ich gern Deutsche Sprache und das Leben in Deutschland kennenlernen. Ich wÃ¤re Ihnen sehr dankbar, wenn Sie mir weitere Informationen Ã¼ber Reche, Pflichen und AufenthaltsformalitÃ¤ten zuschicken kÃ¶nnten.\n",
      "Werde ich versichert? Welche FreizeitaktivitÃ¤ten finden statt? Wiviel Stunden in der Woche mÃ¼ssen die Kinder betreut werden? Ob ich was verdiene?\n",
      "Ãœber eine baldige Antwort wÃ¼rde ich mich sehr freuen\n",
      "Mit freundlichen GrÃ¼ssen\n",
      "Maria Meier\n",
      "\n",
      "ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´ í™•ì¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"UniversalCEFR/merlin_de\", split=\"train\")\n",
    "print(ds)\n",
    "print(ds[0])\n",
    "\n",
    "print(f\"\\n=== UniversalCEFR ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(ds):,}ê°œ\")\n",
    "print(f\"ì»¬ëŸ¼: {ds.column_names}\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "print(f\"  ì œëª©: {ds[0]['title']}\")\n",
    "print(f\"  ì–¸ì–´: {ds[0]['lang']}\")\n",
    "print(f\"  ì¹´í…Œê³ ë¦¬: {ds[0]['category']}\")\n",
    "print(f\"  ë ˆë²¨: {ds[0]['cefr_level']}\")\n",
    "print(f\"  í…ìŠ¤íŠ¸: {ds[0]['text']}\")\n",
    "\n",
    "print(\"\\në°ì´í„°ì…‹ ë©”íƒ€ì •ë³´ í™•ì¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69629e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"de\", clean=True)\n",
    "def split_sentences(text: str):\n",
    "    sents = seg.segment(text)\n",
    "    # ê¸¸ì´ í•„í„° ë“± í›„ì²˜ë¦¬\n",
    "    return [s.strip() for s in sents if 5 <= len(s.split()) <= 80]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c8d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± (2~3ë¬¸ì¥)\n",
    "def make_windows(sents, win=2, stride=1):\n",
    "    out = []\n",
    "    for i in range(0, len(sents) - win + 1, stride):\n",
    "        out.append(\" \".join(sents[i : i + win]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ded7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_instances_windows_only(\n",
    "    doc_id: str,\n",
    "    text: str,\n",
    "    doc_label: str,\n",
    "    split_sentences,\n",
    "    make_windows,\n",
    "    wins=(2, 3),\n",
    "    stride=1,\n",
    "    min_tokens=5,\n",
    "    max_tokens=120,\n",
    "    add_numeric_level=True\n",
    "):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì€ ì €ì¥í•˜ì§€ ì•Šê³ , 2~3ë¬¸ì¥ ìœˆë„ìš°ë§Œ ìƒì„±í•˜ì—¬ ë°˜í™˜.\n",
    "    - doc_labelì€ window ë ˆë²¨ì—ë„ ê·¸ëŒ€ë¡œ ì „íŒŒ (label_doc ìœ ì§€)\n",
    "    - add_numeric_level=Trueë©´ ë ˆë²¨ì„ ìˆ˜ì¹˜í™”í•œ label_numë„ ì¶”ê°€ (ì˜ˆ: A1=1, A1+=1.5, A2=2, ...)\n",
    "    \"\"\"\n",
    "    def _tok_len(t: str) -> int:\n",
    "        return len(t.split())\n",
    "\n",
    "    # CEFR ë ˆë²¨ ìˆ˜ì¹˜í™”(ì„ íƒ)\n",
    "    level2num = {\n",
    "        \"A1\":1.0, \"A1+\":1.5,\n",
    "        \"A2\":2.0, \"A2+\":2.5,\n",
    "        \"B1\":3.0, \"B1+\":3.5,\n",
    "        \"B2\":4.0, \"B2+\":4.5,\n",
    "        \"C1\":5.0, \"C2\":6.0\n",
    "    }\n",
    "    label_num = level2num.get(doc_label, None) if add_numeric_level else None\n",
    "\n",
    "    instances = []\n",
    "    sents = split_sentences(text)\n",
    "\n",
    "    for w in wins:\n",
    "        wins_out = make_windows(sents, win=w, stride=stride)\n",
    "        for j, item in enumerate(wins_out):\n",
    "            # make_windowsê°€ (span_idxs, text) ë˜ëŠ” text ë§Œ ë°˜í™˜í•˜ëŠ” ë‘ ì¼€ì´ìŠ¤ ëª¨ë‘ ì§€ì›\n",
    "            if isinstance(item, tuple) and len(item) == 2:\n",
    "                span, wtext = item\n",
    "            else:\n",
    "                span, wtext = None, item\n",
    "            if min_tokens <= _tok_len(wtext) <= max_tokens:\n",
    "                row = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"unit\": f\"win{w}\",\n",
    "                    \"idx\": j,\n",
    "                    \"text\": wtext,\n",
    "                    \"label_doc\": doc_label,     # ë¬¸ì„œ ë¼ë²¨ ìœ ì§€\n",
    "                    \"span\": span                # ì› ë¬¸ì¥ ì¸ë±ìŠ¤ ë²”ìœ„ (ìˆì„ ê²½ìš°)\n",
    "                }\n",
    "                if label_num is not None:\n",
    "                    row[\"label_num\"] = label_num\n",
    "                instances.append(row)\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6214e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ë„¤ êµ¬í˜„ì²´ import (split_sentences, make_windows)\n",
    "# from your_module import split_sentences, make_windows\n",
    "\n",
    "def process_to_windows_only(\n",
    "    ds_iter,\n",
    "    text_key: str = \"text\",\n",
    "    label_key: str = \"cefr_level\",\n",
    "    id_key: str | None = None,\n",
    "    max_docs: int | None = None,\n",
    "    **doc_kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    HF dataset iterableì„ ë°›ì•„ ìœˆë„ìš° ì¸ìŠ¤í„´ìŠ¤ë§Œ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    for k, rec in enumerate(ds_iter):\n",
    "        if max_docs is not None and k >= max_docs:\n",
    "            break\n",
    "        doc_id = rec[id_key] if id_key and id_key in rec else f\"doc_{k}\"\n",
    "        text = rec[text_key]\n",
    "        label = rec[label_key]\n",
    "        rows = doc_to_instances_windows_only(\n",
    "            doc_id=doc_id,\n",
    "            text=text,\n",
    "            doc_label=label,\n",
    "            split_sentences=split_sentences,\n",
    "            make_windows=make_windows,\n",
    "            **doc_kwargs\n",
    "        )\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty and {\"doc_id\",\"unit\",\"idx\"}.issubset(df.columns):\n",
    "        df = df.sort_values([\"doc_id\",\"unit\",\"idx\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def save_parquet_jsonl(df: pd.DataFrame, base_path: str):\n",
    "    os.makedirs(os.path.dirname(base_path), exist_ok=True)\n",
    "    pq = base_path + \".parquet\"\n",
    "    jl = base_path + \".jsonl\"\n",
    "    df.to_parquet(pq, index=False)\n",
    "    with open(jl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in df.to_dict(orient=\"records\"):\n",
    "            import json\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"âœ… Saved: {pq} | rows={len(df):,}\")\n",
    "    print(f\"âœ… Saved: {jl} | rows={len(df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c455dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def verify_windows_df(df: pd.DataFrame, preview=8):\n",
    "    assert set(df[\"unit\"].unique()) <= {\"win2\",\"win3\"}, \"âš ï¸ sent í–‰ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. (unit ì»¬ëŸ¼ í™•ì¸)\"\n",
    "\n",
    "    print(\"ğŸ§¾ Preview\")\n",
    "    print(df[[\"doc_id\",\"unit\",\"idx\",\"label_doc\",\"text\"]].head(preview))\n",
    "\n",
    "    print(\"\\nğŸ“Š Unit counts\")\n",
    "    print(df[\"unit\"].value_counts())\n",
    "\n",
    "    # ê¸¸ì´ í†µê³„\n",
    "    lens = df[\"text\"].str.split().apply(len)\n",
    "    print(\"\\nğŸ“ˆ Token length stats\")\n",
    "    print(f\"- mean: {lens.mean():.1f}, min: {lens.min()}, max: {lens.max()}\")\n",
    "\n",
    "    # ë¼ë²¨ ë¶„í¬\n",
    "    print(\"\\nğŸ·ï¸ Label distribution (label_doc)\")\n",
    "    print(df[\"label_doc\"].value_counts())\n",
    "\n",
    "    # ë¬¸ì„œë³„ ì»¤ë²„ë¦¬ì§€ ìƒ˜í”Œ\n",
    "    sample_doc = df[\"doc_id\"].iloc[0]\n",
    "    print(f\"\\nğŸ‘€ Sample windows for doc_id={sample_doc}\")\n",
    "    print(df[df[\"doc_id\"] == sample_doc].head(10)[[\"unit\",\"idx\",\"text\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cac1d2",
   "metadata": {},
   "source": [
    "## ë¶„í• ëœ ë°ì´í„° ê²€í† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4460a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: outputs/parsed/ucefr_elg_en_windows_only.parquet | rows=5,996\n",
      "âœ… Saved: outputs/parsed/ucefr_elg_en_windows_only.jsonl | rows=5,996\n",
      "ğŸ§¾ Preview\n",
      "  doc_id  unit  idx label_doc  \\\n",
      "0  doc_0  win2    0        B2   \n",
      "1  doc_0  win2    1        B2   \n",
      "2  doc_0  win2    2        B2   \n",
      "3  doc_0  win3    0        B2   \n",
      "4  doc_0  win3    1        B2   \n",
      "5  doc_0  win3    2        B2   \n",
      "6  doc_1  win2    0        B2   \n",
      "7  doc_1  win2    1        B2   \n",
      "8  doc_1  win2    2        B2   \n",
      "9  doc_1  win2    3        B2   \n",
      "\n",
      "                                                text  \n",
      "0  Sehr geehrte Damen und Herren, ihre Anzeige in...  \n",
      "1  Ich bin flexibel und fÃ¼r neuen Aufgaben ofen. ...  \n",
      "2  Naturlich wÃ¼rde ich gern Deutsche Sprache und ...  \n",
      "3  Sehr geehrte Damen und Herren, ihre Anzeige in...  \n",
      "4  Ich bin flexibel und fÃ¼r neuen Aufgaben ofen. ...  \n",
      "5  Naturlich wÃ¼rde ich gern Deutsche Sprache und ...  \n",
      "6  Sehr geehrte Damen und Herren ich habe Ihre An...  \n",
      "7  FÃ¼r diese Stelle bringe ich alle Voraussetzung...  \n",
      "8  Diese Erfarung wurde mich spÃ¤te gut helfen, be...  \n",
      "9  Kann ich mit meinem Armenischen Pass nach Deut...  \n",
      "\n",
      "ğŸ“Š Unit counts\n",
      "unit\n",
      "win2    3163\n",
      "win3    2833\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Token length stats\n",
      "- mean: 29.1, min: 10, max: 115\n",
      "\n",
      "ğŸ·ï¸ Label distribution (label_doc)\n",
      "label_doc\n",
      "B2    2579\n",
      "B1    1990\n",
      "A2     933\n",
      "C1     372\n",
      "A1      86\n",
      "C2      36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ Sample windows for doc_id=doc_0\n",
      "   unit  idx                                               text\n",
      "0  win2    0  Sehr geehrte Damen und Herren, ihre Anzeige in...\n",
      "1  win2    1  Ich bin flexibel und fÃ¼r neuen Aufgaben ofen. ...\n",
      "2  win2    2  Naturlich wÃ¼rde ich gern Deutsche Sprache und ...\n",
      "3  win3    0  Sehr geehrte Damen und Herren, ihre Anzeige in...\n",
      "4  win3    1  Ich bin flexibel und fÃ¼r neuen Aufgaben ofen. ...\n",
      "5  win3    2  Naturlich wÃ¼rde ich gern Deutsche Sprache und ...\n"
     ]
    }
   ],
   "source": [
    "# 1) ë°ì´í„° ë¡œë“œ\n",
    "ds = load_dataset(\"UniversalCEFR/merlin_de\", split=\"train\")\n",
    "\n",
    "# 2) ìœˆë„ìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¬¸ì¥ ì €ì¥ X)\n",
    "df_win = process_to_windows_only(\n",
    "    ds_iter=ds,\n",
    "    text_key=\"text\",\n",
    "    label_key=\"cefr_level\",\n",
    "    id_key=None,          # ë°ì´í„°ì— ê³ ìœ  id ìˆìœ¼ë©´ ì§€ì •\n",
    "    max_docs=None,        # ì¼ë¶€ë§Œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ìˆ«ì\n",
    "    wins=(2,3),           # 2~3ë¬¸ì¥ ìœˆë„ìš°\n",
    "    stride=3,         # ì¤‘ë³µ ì„¤ì •\n",
    "    min_tokens=5,\n",
    "    max_tokens=120,\n",
    "    add_numeric_level=True\n",
    ")\n",
    "\n",
    "# 3) ì €ì¥\n",
    "base = \"outputs/parsed/ucefr_elg_en_windows_only\"\n",
    "save_parquet_jsonl(df_win, base)\n",
    "\n",
    "# 4) ê²€ì¦\n",
    "verify_windows_df(df_win, preview=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ea0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_win"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
