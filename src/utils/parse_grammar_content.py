"""
ë¬¸ë²• ì½˜í…ì¸  HTML íŒŒì‹± ë° êµ¬ì¡°í™”

Deutsch im Blick ë¬¸ë²• í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¡œ ë³€í™˜
"""

import os
import json
import re
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict


@dataclass
class GrammarTopic:
    """ë¬¸ë²• ì£¼ì œ ë°ì´í„° êµ¬ì¡°"""
    id: str
    title: str
    cefr_level: str  # A1, A2, B1, B2, C1
    category: str  # verbs, nouns, adjectives, prepositions, etc.
    subcategory: str
    tags: List[str]
    difficulty_score: float  # 1.0-5.0
    estimated_time: int  # ë¶„
    prerequisites: List[str]
    content: Dict[str, any]
    source_file: str


class GrammarContentParser:
    """ë¬¸ë²• ì½˜í…ì¸  íŒŒì„œ"""

    # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘
    CATEGORY_KEYWORDS = {
        'verbs': ['verb', 'verben', 'perfekt', 'prÃ¤teritum', 'konjugation'],
        'nouns': ['nomen', 'noun', 'substantiv', 'plural'],
        'adjectives': ['adjektiv', 'adjective'],
        'articles': ['artikel', 'article', 'der', 'die', 'das'],
        'prepositions': ['prÃ¤position', 'preposition'],
        'pronouns': ['pronomen', 'pronoun'],
        'cases': ['kasus', 'case', 'nominativ', 'akkusativ', 'dativ', 'genitiv'],
        'conjunctions': ['konjunktion', 'conjunction'],
        'word_order': ['wortstellung', 'word order', 'syntax'],
        'vocabulary': ['wortschatz', 'vocabulary']
    }

    # CEFR ë ˆë²¨ ì¶”ì • í‚¤ì›Œë“œ
    CEFR_KEYWORDS = {
        'A1': ['basic', 'introduction', 'simple', 'sein', 'haben', 'present tense'],
        'A2': ['regular', 'plural', 'perfect tense', 'modal verbs', 'personal pronoun'],
        'B1': ['past tense', 'conjunctions', 'relative clause', 'passive', 'subjunctive'],
        'B2': ['complex', 'advanced', 'indirect speech', 'subjunctive II'],
        'C1': ['sophisticated', 'nuanced', 'literary', 'formal']
    }

    def __init__(self, html_dir: str):
        self.html_dir = Path(html_dir)
        self.topics: List[GrammarTopic] = []

    def parse_all(self) -> List[GrammarTopic]:
        """ëª¨ë“  HTML íŒŒì¼ íŒŒì‹±"""
        html_files = sorted(self.html_dir.glob("*.html"))
        print(f"ğŸ“š {len(html_files)}ê°œ íŒŒì¼ íŒŒì‹± ì‹œì‘...")

        for html_file in html_files:
            try:
                topic = self.parse_file(html_file)
                if topic:
                    self.topics.append(topic)
                    print(f"  âœ“ {topic.id} ({topic.category})")
            except Exception as e:
                print(f"  âœ— {html_file.name}: {e}")

        print(f"\nâœ… ì´ {len(self.topics)}ê°œ ì£¼ì œ íŒŒì‹± ì™„ë£Œ")
        return self.topics

    def parse_file(self, file_path: Path) -> Optional[GrammarTopic]:
        """ë‹¨ì¼ HTML íŒŒì¼ íŒŒì‹±"""
        with open(file_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('title')
        if not title_elem:
            return None

        title = title_elem.text.strip()
        # "â€“ Grammar to Accompany Deutsch im Blick" ì œê±°
        title = re.sub(r'\s*[â€“-]\s*Grammar to Accompany.*', '', title)
        title = title.strip()

        if not title or 'table of contents' in title.lower():
            return None

        # IDëŠ” íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        topic_id = file_path.stem

        # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ
        content_div = soup.find('div', class_='entry-content') or soup.find('main')
        if not content_div:
            return None

        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        full_text = content_div.get_text().lower()

        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = self._classify_category(title, full_text)
        subcategory = self._extract_subcategory(title)

        # CEFR ë ˆë²¨ ì¶”ì •
        cefr_level = self._estimate_cefr_level(title, full_text, category)

        # íƒœê·¸ ì¶”ì¶œ
        tags = self._extract_tags(title, full_text)

        # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
        difficulty = self._calculate_difficulty(cefr_level, category)

        # ì˜ˆìƒ í•™ìŠµ ì‹œê°„
        estimated_time = self._estimate_time(full_text)

        # ì½˜í…ì¸  êµ¬ì¡°í™”
        content = self._extract_content(content_div)

        return GrammarTopic(
            id=topic_id,
            title=title,
            cefr_level=cefr_level,
            category=category,
            subcategory=subcategory,
            tags=tags,
            difficulty_score=difficulty,
            estimated_time=estimated_time,
            prerequisites=[],  # ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€
            content=content,
            source_file=file_path.name
        )

    def _classify_category(self, title: str, text: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()

        for category, keywords in self.CATEGORY_KEYWORDS.items():
            for keyword in keywords:
                if keyword in title_lower or keyword in text[:500]:
                    return category

        return 'other'

    def _extract_subcategory(self, title: str) -> str:
        """í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ
        match = re.search(r'\((.*?)\)', title)
        if match:
            return match.group(1).strip()

        # ì½œë¡  ë’¤ì˜ ë‚´ìš©
        if ':' in title:
            return title.split(':')[-1].strip()

        return ''

    def _estimate_cefr_level(self, title: str, text: str, category: str) -> str:
        """CEFR ë ˆë²¨ ì¶”ì •"""
        title_lower = title.lower()

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        scores = {level: 0 for level in self.CEFR_KEYWORDS.keys()}

        for level, keywords in self.CEFR_KEYWORDS.items():
            for keyword in keywords:
                if keyword in title_lower:
                    scores[level] += 3
                if keyword in text[:1000]:
                    scores[level] += 1

        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë ˆë²¨
        category_defaults = {
            'articles': 'A1',
            'nouns': 'A1',
            'verbs': 'A2',
            'adjectives': 'A2',
            'prepositions': 'A2',
            'cases': 'B1',
            'conjunctions': 'B1',
            'pronouns': 'A2'
        }

        # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë ˆë²¨ ì„ íƒ
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)

        # ê¸°ë³¸ê°’
        return category_defaults.get(category, 'A2')

    def _extract_tags(self, title: str, text: str) -> List[str]:
        """íƒœê·¸ ì¶”ì¶œ"""
        tags = []

        # ì œëª©ì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        title_words = re.findall(r'\b[A-Za-zÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ]{4,}\b', title)
        tags.extend(title_words[:3])

        # íŠ¹ìˆ˜ í‚¤ì›Œë“œ
        special_keywords = ['endings', 'tense', 'case', 'gender', 'plural',
                          'conjugation', 'declension', 'irregular']
        for keyword in special_keywords:
            if keyword in text[:500]:
                tags.append(keyword)

        return list(set(tags))[:5]

    def _calculate_difficulty(self, cefr_level: str, category: str) -> float:
        """ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (1.0-5.0)"""
        base_scores = {
            'A1': 1.0,
            'A2': 2.0,
            'B1': 3.0,
            'B2': 4.0,
            'C1': 5.0
        }

        category_modifiers = {
            'articles': -0.2,
            'nouns': -0.1,
            'verbs': 0.2,
            'cases': 0.3,
            'conjunctions': 0.2,
            'word_order': 0.3
        }

        score = base_scores.get(cefr_level, 2.5)
        score += category_modifiers.get(category, 0)

        return round(max(1.0, min(5.0, score)), 1)

    def _estimate_time(self, text: str) -> int:
        """ì˜ˆìƒ í•™ìŠµ ì‹œê°„ ê³„ì‚° (ë¶„)"""
        word_count = len(text.split())

        # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ì‹œê°„ ì¶”ì •
        if word_count < 300:
            return 10
        elif word_count < 600:
            return 15
        elif word_count < 1000:
            return 20
        else:
            return 30

    def _extract_content(self, content_div) -> Dict:
        """ì½˜í…ì¸  êµ¬ì¡°í™”"""
        # ì œëª©ë“¤ ì¶”ì¶œ
        headings = []
        for h in content_div.find_all(['h2', 'h3', 'h4']):
            headings.append({
                'level': h.name,
                'text': h.get_text().strip()
            })

        # ì˜ˆì œ ë¬¸ì¥ ì¶”ì¶œ (ë…ì¼ì–´ ë¬¸ì¥ íŒ¨í„´)
        examples = []
        for p in content_div.find_all('p'):
            text = p.get_text().strip()
            # ë…ì¼ì–´ ë¬¸ì¥ íŒ¨í„´ (ëŒ€ë¬¸ì ì‹œì‘, ë§ˆì¹¨í‘œ ë)
            if re.match(r'^[A-ZÃ„Ã–Ãœ].*[.!?]$', text) and len(text) < 200:
                examples.append(text)

        # í…Œì´ë¸” ì¶”ì¶œ (ê²©ë³€í™”í‘œ ë“±)
        tables = []
        for table in content_div.find_all('table'):
            table_data = []
            for row in table.find_all('tr'):
                cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]
                if cells:
                    table_data.append(cells)
            if table_data:
                tables.append(table_data)

        return {
            'headings': headings[:10],
            'examples': examples[:20],
            'tables': tables[:5],
            'full_html': str(content_div)[:5000]  # ì²˜ìŒ 5000ìë§Œ
        }

    def save_to_json(self, output_file: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        data = [asdict(topic) for topic in self.topics]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ì´ {len(self.topics)}ê°œ ì£¼ì œ")

    def save_summary(self, output_file: str):
        """ìš”ì•½ ì •ë³´ ì €ì¥"""
        summary = {
            'total_topics': len(self.topics),
            'by_category': {},
            'by_cefr_level': {},
            'topics': []
        }

        # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        for topic in self.topics:
            summary['by_category'][topic.category] = \
                summary['by_category'].get(topic.category, 0) + 1
            summary['by_cefr_level'][topic.cefr_level] = \
                summary['by_cefr_level'].get(topic.cefr_level, 0) + 1

            summary['topics'].append({
                'id': topic.id,
                'title': topic.title,
                'category': topic.category,
                'cefr_level': topic.cefr_level,
                'difficulty': topic.difficulty_score,
                'time': topic.estimated_time
            })

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“Š ìš”ì•½ ì €ì¥ ì™„ë£Œ: {output_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    project_root = Path(__file__).parent.parent.parent
    html_dir = project_root / "notebooks" / "pressbook_html"
    output_dir = project_root / "data" / "grammar_content"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    # íŒŒì‹± ì‹¤í–‰
    parser = GrammarContentParser(html_dir)
    topics = parser.parse_all()

    # ê²°ê³¼ ì €ì¥
    parser.save_to_json(output_dir / "grammar_topics.json")
    parser.save_summary(output_dir / "grammar_summary.json")

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“ˆ í†µê³„:")
    print(f"  ì¹´í…Œê³ ë¦¬ë³„:")
    category_counts = {}
    cefr_counts = {}
    for topic in topics:
        category_counts[topic.category] = category_counts.get(topic.category, 0) + 1
        cefr_counts[topic.cefr_level] = cefr_counts.get(topic.cefr_level, 0) + 1

    for cat, count in sorted(category_counts.items()):
        print(f"    {cat}: {count}ê°œ")

    print(f"\n  CEFR ë ˆë²¨ë³„:")
    for level in ['A1', 'A2', 'B1', 'B2', 'C1']:
        if level in cefr_counts:
            print(f"    {level}: {cefr_counts[level]}ê°œ")


if __name__ == "__main__":
    main()
